{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"18d4044a5a94451d967ea63e5b26cdf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f93a17ea146549c39ae08d19d3b2e6b3","IPY_MODEL_156702951ba4446782c043ccc5279ebb","IPY_MODEL_ea4fb237aba34f2b974885717a07e175"],"layout":"IPY_MODEL_31110e9f25d3468db3595441cca204ac"}},"f93a17ea146549c39ae08d19d3b2e6b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50cd42307f3149729b173d5f117dacc0","placeholder":"​","style":"IPY_MODEL_4fc177f1140f4c3f90b3cfc54e33e98d","value":"100%"}},"156702951ba4446782c043ccc5279ebb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e0b931a2eca4962a256a5c7db7431cb","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_820ae3ccaaf94358b2e06ab318a1ab3f","value":553433881}},"ea4fb237aba34f2b974885717a07e175":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d1d9e2101414f9aa28b6c02872a2ef8","placeholder":"​","style":"IPY_MODEL_991d741003974d82ab91e957fbf7fd38","value":" 528M/528M [00:02&lt;00:00, 218MB/s]"}},"31110e9f25d3468db3595441cca204ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50cd42307f3149729b173d5f117dacc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fc177f1140f4c3f90b3cfc54e33e98d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e0b931a2eca4962a256a5c7db7431cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820ae3ccaaf94358b2e06ab318a1ab3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d1d9e2101414f9aa28b6c02872a2ef8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"991d741003974d82ab91e957fbf7fd38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ESE 5390 Project Code\n\nThe provided project starter code walks through a comparative study of three post-training pruning methods:\n\n* **Global Weight Pruning**\n* **Layer-wise Weight Pruning**\n* **Layer-wise Output Channel Pruning**\n\nFollowing this starter code, you will further enhance the **efficiency and accuracy** of layer-wise channel pruning by implementing one of the two advanced techniques:\n\n* **Training-Aware Pruning:**\n    Apply regularization during training to promote sparsity. Refer to Wen et al.’s _\"Learning Structured Sparsity in Deep Neural Networks\"_, though you are encouraged to explore other regularization techniques.\n\n* **Iterative Pruning:**\n    Repeatedly prune and retrain the network to restore accuracy. You may refer to Han et al.’s _\"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding.\"_ However, feel free to develop innovative variations of this approach.\n\nThe grading will reflect how well you understand the literature, design and execute experiments, present your findings, and document your project in a comprehensive technical report.\n\n **Bonus points** are available for extending the project scope through additional explorations (see **Bonus Opportunities**).\n\n## Environment\n\nWe strongly recommend using GPU or TPU to avoid excessive runtimes.","metadata":{"id":"p3qDRoUU17LJ"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.utils.prune as prune\nimport torchvision\nimport time\nimport numpy as np\nimport copy\nimport sys\nimport os\nfrom torchvision import datasets, models, transforms\nfrom pathlib import Path\nimport random\nimport matplotlib.pyplot as plt\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"Torchvision Version: \", torchvision.__version__)\nPath(\"./data\").mkdir(parents=True, exist_ok=True)\ndata_dir = Path('./data')\nnet_fn = Path('./net')\n\n# see if gdown is already downloaded\ntry:\n    import gdown\n    print(\"gdown is already installed, skipping installing command to save time......\")\nexcept ImportError:\n    !conda install -y gdown\n\n# Seed for reproducibility \ntorch.manual_seed(0)\nnp.random.seed(0)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIsiBQXk17LK","outputId":"fcb92ed1-27df-4870-f98c-aa1be64aa17b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"PyTorch Version: \", torch.__version__)\nprint(\"Torchvision Version: \", torchvision.__version__)\nPath(\"./data\").mkdir(parents=True, exist_ok=True)\ndata_dir = Path('./data')\nnet_fn = Path('./net')\n\n# see if gdown is already downloaded\ntry:\n    import gdown\n    print(\"gdown is already installed, skipping installing command to save time......\")\nexcept ImportError:\n    !conda install -y gdown","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tu38ZXnh17LL","outputId":"81a329be-c507-42bd-943d-7238bd1f9ea8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"REPOS = [\n    [\n        \"1ZLyAcimq4sdZ0tl5yINudwSD843pAoOJ\",\n        \"1EXwrSw6BWKMC4ovPRUyfuqeObuqsR-R5\",\n        \"1rFIAJ9aLZrRCOeijo5mI0zWiEl3OMJ3j\"\n    ],\n    [\n        \"1VLe11mOwsetC4IlL3wy8QexTat8XCool\",\n        \"1_QwUcr3gmnjPFbKj0VrfOayVEWjRBS4Z\",\n        \"1fgf9elhD7EhbKJn2NMnQS1iwQXesQZ3g\"\n    ],\n    [\n        \"1BKC4kCB9sbwfRuhrAJWVEmbLoHx2vJWC\",\n        \"1qhrcYgGKRf3Wt8YDU6aVRhzJDFHeqerP\",\n        \"1JXym04uAoNGGpkBRzIn0yStZmRX0SmNu\"\n    ]\n]\n\nFILES = [\n    Path(\"./data/ILSVRC2012_devkit_t12.tar.gz\"),\n    Path(\"./data/ILSVRC2012_devkit_t3.tar.gz\"),\n    Path(\"./data/ILSVRC2012_img_val.tar\")\n]\n\n# Shuffle the order of repos\nrandom.seed(int(time.time()))\nrandom.shuffle(REPOS)\n\nfor repo_index, repo in enumerate(REPOS):\n    missing_files = [str(file) for file in FILES if not file.is_file()]\n    if not missing_files:\n        print(\"All files are present. Skipping further downloads.\")\n        break\n\n    print(f\"Attempting download from repo {repo_index}\")\n    print(f\"Missing files: {missing_files}\")\n\n    for file, url in zip(FILES, repo):\n        if not file.is_file():\n            print(f\"Downloading {file}...\")\n            !gdown \"{url}\" -O \"{str(file)}\"\n\nmissing_files = [str(file) for file in FILES if not file.is_file()]\nif missing_files:\n    print(f\"Failed to download all files. Still missing: {missing_files}\")\n    print(f\"Contact TA team through EdStem with the output of this code block!\", file=sys.stderr)\nelse:\n    print(\"All files successfully downloaded.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create transform to preprocess data\nval_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Create validation dataset\nval_dataset = datasets.ImageNet('./data', split='val', transform=val_transform)\n\n# Create validation dataloader\nval_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, num_workers=2)\n\nprint(f'Number of validation images: {len(val_dataset)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOLsgH-k17LL","outputId":"c4d853d5-0cd6-49ed-fa8d-9b42d203ef3b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Put your reusable functions here.\n# You can copy functions from previous labs and tutorials.","metadata":{"id":"Z8_5gmh517LM","colab":{"base_uri":"https://localhost:8080/","height":937,"referenced_widgets":["18d4044a5a94451d967ea63e5b26cdf5","f93a17ea146549c39ae08d19d3b2e6b3","156702951ba4446782c043ccc5279ebb","ea4fb237aba34f2b974885717a07e175","31110e9f25d3468db3595441cca204ac","50cd42307f3149729b173d5f117dacc0","4fc177f1140f4c3f90b3cfc54e33e98d","2e0b931a2eca4962a256a5c7db7431cb","820ae3ccaaf94358b2e06ab318a1ab3f","0d1d9e2101414f9aa28b6c02872a2ef8","991d741003974d82ab91e957fbf7fd38"]},"outputId":"786da2d0-f2fc-4f61-a447-00c0763ad2f5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Global Weight Pruning\n\n- Perform pruning using `prune.global_unstructured` on a pretrained VGG16 using various sparsity targets.\n- The models should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n- Save a copy of each pruned model in a dictionary.\n- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%).","metadata":{"id":"BA9VNPvg17LM"}},{"cell_type":"code","source":"orig_model = #TODO: load pretrained model\nprune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\nglobal_pruning = {} # Dictionary to store global pruning results\nfor prune_rate in prune_rate_list:\n    model = #TODO: load pretrained model\n    parameter_to_prune = (\n        (models.features[0], 'weight'), # conv1 of VGG16\n        #TODO: Add more layers to prune\n    )\n    #TODO: Prune model\n    #TODO: make the pruning permanent to increase speed\n    global_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n    global_pruning[prune_rate]['model'] = model # Copy pruned model to dictionary\n    # TODO Run validation on the pruned model\n    global_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n    global_pruning[prune_rate]['top1_acc_rel'] = None # Percent accuracy compared to original model\n    print(f'Top1 accuracy for prune amount {prune_rate}%: {global_pruning[prune_rate][\"top1_acc\"]}%')\n    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {global_pruning[prune_rate][\"top1_acc_rel\"]}%')\n\n# TODO plot the results","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iI2sMPnUXxJG","outputId":"5ef2dc94-4d78-480f-bb5d-50bda996a1c1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Layer-wise Weight Pruning\n\n- Perform pruning using `prune.l1_unstructured` on each layer of pretrained VGG16 using various sparsity targets.\n- Each layer should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n- Save a copy of each pruned model in a dictionary.\n- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%).\n- What do you observe? Why does layer-wise pruning perform better/worse than global pruning?","metadata":{"id":"7BjkfRbx17LP"}},{"cell_type":"code","source":"orig_model = #TODO: load pretrained model\nprune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\nlayer_pruning = {} # Dictionary to store layer pruning results\nfor prune_rate in prune_rate_list:\n    model = #TODO: load pretrained model\n    convs_to_prune = () #TODO: Add conv layers to prune\n    linears_to_prune = () #TODO: Add linear layers to prune\n    #TODO: Prune model\n    layer_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n    layer_pruning[prune_rate]['model'] = model # Copy pruned model to dictionary\n    # TODO Run validation on the pruned model\n    layer_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n    layer_pruning[prune_rate]['top1_acc_rel'] = None # Percent accuracy compared to original model\n    print(f'Top1 accuracy for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc\"]}%')\n    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {layer_pruning[prune_rate][\"top1_acc_rel\"]}%')\n\n# TODO plot the results","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDMDWsVp0_ih","outputId":"ae446451-df7e-44ed-9d80-2a29d32e66af","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Layer-wise Output Channel Pruning\n\n- Perform pruning using `prune.ln_structured` on each layer of pretrained VGG16 using various sparsity targets.\n- Prune along the output channels for conv layers and output dimension for linear layers\n- Do not prune the last linear layer to preserve the number of predicted classes\n- Each layer should be pruned away at 5% increments from 5% to 45% total pruning of the model.\n- Save a copy of each pruned model in a dictionary.\n- Plot the top-1 accuracy against weight pruned from 0% to 45% (use the original VGG16 for 0%).\n- What do you observe? Why does layer-wise channel pruning perform better/worse than layer-wise unstructured pruning?","metadata":{"id":"CjLftt6317LQ"}},{"cell_type":"code","source":"orig_model = #TODO: load pretrained model\nprune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\nchannel_pruning = {} # Dictionary to store channel pruning results\nfor prune_rate in prune_rate_list:\n    model = #TODO: load pretrained model\n    convs_to_prune = () #TODO: Add conv layers to prune\n    linears_to_prune = () #TODO: Add linear layers to prune (except the last linear layer)\n    #TODO: Prune model\n    channel_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n    channel_pruning[prune_rate]['model'] = model # Copy pruned model to dictionary\n    # TODO Run validation on the pruned model\n    channel_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n    channel_pruning[prune_rate]['top1_acc_rel'] = None # Percent accuracy compared to original model\n    print(f'Top1 accuracy for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc\"]}%')\n    print(f'Top1 accuracy (rel) for prune amount {prune_rate}%: {channel_pruning[prune_rate][\"top1_acc_rel\"]}%')\n\n# TODO plot the results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.5 Layer-wise Output Channel Pruning (Continued)\n\nIn this section, we harden the channel pruning by removing the channels\n- Collect the number of non-zero output channels for each layer after `prune.ln_structured`\n- Instantiate a new model based on the number of non-zero output channels\n- Remember that the input channels of next layer should match the output channels for current layer\n- Pay special attention to the last convolution layer and first linear layer\n- Copy over the the non-zero weights from the pruned model to the hardened model\n- Calculate the run time and model size for the original VGG16 and the hardened VGG16s\n- Plot the **relative** top-1 accuracy, run time and model size from 0% to 45% pruned VGG16\n- What do you observe? What is the trade-off between accuracy, run time, and model size?","metadata":{"id":"lv4Zv82h17LQ"}},{"cell_type":"code","source":"prune_rate_list = range(5, 50, 5) # List of prune rate to test from 5-45 inclusive with step of 5\nhardened_pruning = {} # Dictionary to store hardened pruning results\nfor prune_rate in prune_rate_list:\n    #TODO: Collect the number of non-zero channels for each layer\n    #TODO: Instantiate a new model based on collected number of non-zero channels\n    hardened_pruning[prune_rate_list] = {} # Dictionary to store accuracy results and model for each prune rate\n    hardened_pruning[prune_rate]['model'] = model # Copy original model to dictionary\n    hardened_pruning[prune_rate]['top1_acc'] = None # TODO fill with top1 accuracy\n    hardened_pruning[prune_rate]['top1_acc_rel'] = None # TODO Percent accuracy compared to original model\n    hardened_pruning[prune_rate]['run_time'] = None # TODO Collect run time of the hardened model\n    hardened_pruning[prune_rate]['run_time_rel'] = None # TODO Collect run time (relative) of the hardened model\n    hardened_pruning[prune_rate]['model_size'] = None # TODO Collect model size of the hardened model\n    hardened_pruning[prune_rate]['model_size_rel'] = None # TODO Collect model size (relative) of the hardened model","metadata":{"id":"RazkYpMisUu7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TODO plot the relative accuracy, relative run time, and relative model size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training-Aware Pruning or Iterative Pruning","metadata":{}},{"cell_type":"markdown","source":"## 5. (Optional) Bonus Exploration","metadata":{}}]}